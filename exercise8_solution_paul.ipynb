{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8: Part-of-Speech tagging using Markov Models\n",
    "\n",
    "We will be doing Part-of-Speech (POS) tagging. This is a common problem in natural language processing that tries to assign grammatical tags to each word in a sentence. \n",
    "Often this task is posed as a sequence labeling task that can be modeled as a Markov chain.\n",
    "\n",
    "Let's consider an example:\n",
    "We have a sentence $Y_{0:T} = $ `Time flies like an arrow.`  and want to predict the corresponding POS tags that we model as a latent sequence $X_{0:T} =$ `NOUN VERB CONJ DET NOUN`. \n",
    "This example already reveals the challenge of this task which requires us to take context information into account, since the word `flies` could also be classified as `NOUN` in another context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following exercise, we will apply the algorithm that we introduced in lecture 15 (slide 36) to this problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Download and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from functools import partial\n",
    "\n",
    " \n",
    "# download the treebank corpus from nltk\n",
    "nltk.download('treebank')\n",
    " \n",
    "# download the universal tagset from nltk\n",
    "nltk.download('universal_tagset')\n",
    " \n",
    "# reading the Treebank tagged sentences\n",
    "nltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))\n",
    "\n",
    "# split data into training and validation set in the ratio 80:20\n",
    "train_set,test_set = train_test_split(nltk_data,train_size=0.80,test_size=0.20,random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80310\n",
      "20366\n"
     ]
    }
   ],
   "source": [
    "# create list of train and test tagged words (ignore sentence boundaries)\n",
    "train_data = [ tup for sent in train_set for tup in sent ]\n",
    "test_data = [ tup for sent in test_set for tup in sent ]\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Drink', 'NOUN'),\n",
       " ('Carrier', 'NOUN'),\n",
       " ('Competes', 'VERB'),\n",
       " ('With', 'ADP'),\n",
       " ('Cartons', 'NOUN'),\n",
       " ('At', 'ADP'),\n",
       " ('last', 'ADJ'),\n",
       " ('count', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Candela', 'NOUN'),\n",
       " ('had', 'VERB'),\n",
       " ('sold', 'VERB'),\n",
       " ('$', '.'),\n",
       " ('4', 'NUM'),\n",
       " ('million', 'NUM'),\n",
       " ('*U*', 'X'),\n",
       " ('of', 'ADP'),\n",
       " ('its', 'PRON'),\n",
       " ('medical', 'ADJ'),\n",
       " ('devices', 'NOUN'),\n",
       " ('in', 'ADP'),\n",
       " ('Japan', 'NOUN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check some of the tagged words.\n",
    "train_data[:23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Carrier', 'NOUN')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NOUN'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use set datatype to check how many unique tags are present in training data\n",
    "tags = {tag for word,tag in train_data}\n",
    "tag_vocab = {tag: i for i, tag in enumerate(tags)}\n",
    " \n",
    "# check total words in vocabulary\n",
    "words = {word for word,tag in train_data}\n",
    "word_vocab = {word: i for i, word in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get tag for noun\n",
    "tag_vocab[train_data[1][1]]\n",
    "# Attention! vocab tags fo not stay the same after restarting notebook kernel\n",
    "# This is not problem and simply reults in a differnt odreder of the matrices/data frames.\n",
    "# Values do not change!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags: {'CONJ', 'NOUN', 'ADP', 'ADV', '.', 'PRON', 'DET', 'PRT', 'VERB', 'ADJ', 'X', 'NUM'}\n",
      "tag_vocab: {'CONJ': 0, 'NOUN': 1, 'ADP': 2, 'ADV': 3, '.': 4, 'PRON': 5, 'DET': 6, 'PRT': 7, 'VERB': 8, 'ADJ': 9, 'X': 10, 'NUM': 11}\n"
     ]
    }
   ],
   "source": [
    "print(f'tags: {tags}')\n",
    "print(f'tag_vocab: {tag_vocab}')\n",
    "\n",
    "#print(f'words: {words}')\n",
    "#print(f'word_vocab: {word_vocab}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Precomputing probabilities (\"Training\")  \n",
    "#### [1] (a) Compute the transition probability\n",
    "First we need to compute the transition probability, i.e. the probability that one POS tag follows on another: $p(x_t|x_{t-1})$\n",
    "\n",
    "For this purpose, we can just pre-compute the entire table of conditional probabilities based on the counts from the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_transition(train_data, tag_vocab):\n",
    "    # Initialize a matrix to store transition probabilities\n",
    "    p_transition = np.zeros((len(tag_vocab), len(tag_vocab)))\n",
    "    \n",
    "    # Count tag transitions in the training data\n",
    "    for i in range(1, len(train_data)):  # start countin at index 1 so we can refer to the tag a position 0 \n",
    "        \n",
    "        # get current and previous tag\n",
    "        current_tag = tag_vocab[train_data[i][1]]                    \n",
    "        previous_tag = tag_vocab[train_data[i - 1][1]]\n",
    "        \n",
    "        # increase transition count from previous_tag to current_tag\n",
    "        p_transition[previous_tag, current_tag] += 1\n",
    "    \n",
    "    # get row-wise sum of transition counts\n",
    "    row_sums = p_transition.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    # normalize counts to get probabilities\n",
    "    p_transition = p_transition / row_sums\n",
    "    \n",
    "    return p_transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row:    Previous tag\n",
      "Column: Current tag\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONJ</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADV</th>\n",
       "      <th>.</th>\n",
       "      <th>PRON</th>\n",
       "      <th>DET</th>\n",
       "      <th>PRT</th>\n",
       "      <th>VERB</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>X</th>\n",
       "      <th>NUM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CONJ</th>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.349067</td>\n",
       "      <td>0.055982</td>\n",
       "      <td>0.057080</td>\n",
       "      <td>0.035126</td>\n",
       "      <td>0.060373</td>\n",
       "      <td>0.123491</td>\n",
       "      <td>0.004391</td>\n",
       "      <td>0.150384</td>\n",
       "      <td>0.113611</td>\n",
       "      <td>0.009330</td>\n",
       "      <td>0.040615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>0.042454</td>\n",
       "      <td>0.262344</td>\n",
       "      <td>0.176827</td>\n",
       "      <td>0.016895</td>\n",
       "      <td>0.240094</td>\n",
       "      <td>0.004659</td>\n",
       "      <td>0.013106</td>\n",
       "      <td>0.043935</td>\n",
       "      <td>0.149134</td>\n",
       "      <td>0.012584</td>\n",
       "      <td>0.028825</td>\n",
       "      <td>0.009144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.323589</td>\n",
       "      <td>0.016958</td>\n",
       "      <td>0.014553</td>\n",
       "      <td>0.038724</td>\n",
       "      <td>0.069603</td>\n",
       "      <td>0.320931</td>\n",
       "      <td>0.001266</td>\n",
       "      <td>0.008479</td>\n",
       "      <td>0.107062</td>\n",
       "      <td>0.034548</td>\n",
       "      <td>0.063275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>0.006982</td>\n",
       "      <td>0.032196</td>\n",
       "      <td>0.119472</td>\n",
       "      <td>0.081458</td>\n",
       "      <td>0.139255</td>\n",
       "      <td>0.012025</td>\n",
       "      <td>0.071373</td>\n",
       "      <td>0.014740</td>\n",
       "      <td>0.339022</td>\n",
       "      <td>0.130721</td>\n",
       "      <td>0.022886</td>\n",
       "      <td>0.029868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.060086</td>\n",
       "      <td>0.218562</td>\n",
       "      <td>0.092918</td>\n",
       "      <td>0.052575</td>\n",
       "      <td>0.092382</td>\n",
       "      <td>0.068777</td>\n",
       "      <td>0.172210</td>\n",
       "      <td>0.002790</td>\n",
       "      <td>0.089700</td>\n",
       "      <td>0.046137</td>\n",
       "      <td>0.025644</td>\n",
       "      <td>0.078219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>0.005011</td>\n",
       "      <td>0.212756</td>\n",
       "      <td>0.022323</td>\n",
       "      <td>0.036902</td>\n",
       "      <td>0.041913</td>\n",
       "      <td>0.006834</td>\n",
       "      <td>0.009567</td>\n",
       "      <td>0.014123</td>\n",
       "      <td>0.484738</td>\n",
       "      <td>0.070615</td>\n",
       "      <td>0.088383</td>\n",
       "      <td>0.006834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>0.000431</td>\n",
       "      <td>0.635906</td>\n",
       "      <td>0.009918</td>\n",
       "      <td>0.012074</td>\n",
       "      <td>0.017393</td>\n",
       "      <td>0.003306</td>\n",
       "      <td>0.006037</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.040247</td>\n",
       "      <td>0.206411</td>\n",
       "      <td>0.045134</td>\n",
       "      <td>0.022855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRT</th>\n",
       "      <td>0.002348</td>\n",
       "      <td>0.250489</td>\n",
       "      <td>0.019569</td>\n",
       "      <td>0.009393</td>\n",
       "      <td>0.045010</td>\n",
       "      <td>0.017613</td>\n",
       "      <td>0.101370</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>0.401174</td>\n",
       "      <td>0.082975</td>\n",
       "      <td>0.012133</td>\n",
       "      <td>0.056751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>0.005433</td>\n",
       "      <td>0.110589</td>\n",
       "      <td>0.092357</td>\n",
       "      <td>0.083886</td>\n",
       "      <td>0.034807</td>\n",
       "      <td>0.035543</td>\n",
       "      <td>0.133610</td>\n",
       "      <td>0.030663</td>\n",
       "      <td>0.167956</td>\n",
       "      <td>0.066390</td>\n",
       "      <td>0.215930</td>\n",
       "      <td>0.022836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>0.016893</td>\n",
       "      <td>0.696893</td>\n",
       "      <td>0.080583</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.066019</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.011456</td>\n",
       "      <td>0.011456</td>\n",
       "      <td>0.063301</td>\n",
       "      <td>0.020971</td>\n",
       "      <td>0.021748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0.010379</td>\n",
       "      <td>0.061695</td>\n",
       "      <td>0.142226</td>\n",
       "      <td>0.025754</td>\n",
       "      <td>0.160869</td>\n",
       "      <td>0.054200</td>\n",
       "      <td>0.056890</td>\n",
       "      <td>0.185086</td>\n",
       "      <td>0.206419</td>\n",
       "      <td>0.017682</td>\n",
       "      <td>0.075726</td>\n",
       "      <td>0.003075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>0.014281</td>\n",
       "      <td>0.351660</td>\n",
       "      <td>0.037487</td>\n",
       "      <td>0.003570</td>\n",
       "      <td>0.119243</td>\n",
       "      <td>0.001428</td>\n",
       "      <td>0.003570</td>\n",
       "      <td>0.026062</td>\n",
       "      <td>0.020707</td>\n",
       "      <td>0.035345</td>\n",
       "      <td>0.202428</td>\n",
       "      <td>0.184220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          CONJ      NOUN       ADP       ADV         .      PRON       DET   \n",
       "CONJ  0.000549  0.349067  0.055982  0.057080  0.035126  0.060373  0.123491  \\\n",
       "NOUN  0.042454  0.262344  0.176827  0.016895  0.240094  0.004659  0.013106   \n",
       "ADP   0.001012  0.323589  0.016958  0.014553  0.038724  0.069603  0.320931   \n",
       "ADV   0.006982  0.032196  0.119472  0.081458  0.139255  0.012025  0.071373   \n",
       ".     0.060086  0.218562  0.092918  0.052575  0.092382  0.068777  0.172210   \n",
       "PRON  0.005011  0.212756  0.022323  0.036902  0.041913  0.006834  0.009567   \n",
       "DET   0.000431  0.635906  0.009918  0.012074  0.017393  0.003306  0.006037   \n",
       "PRT   0.002348  0.250489  0.019569  0.009393  0.045010  0.017613  0.101370   \n",
       "VERB  0.005433  0.110589  0.092357  0.083886  0.034807  0.035543  0.133610   \n",
       "ADJ   0.016893  0.696893  0.080583  0.005243  0.066019  0.000194  0.005243   \n",
       "X     0.010379  0.061695  0.142226  0.025754  0.160869  0.054200  0.056890   \n",
       "NUM   0.014281  0.351660  0.037487  0.003570  0.119243  0.001428  0.003570   \n",
       "\n",
       "           PRT      VERB       ADJ         X       NUM  \n",
       "CONJ  0.004391  0.150384  0.113611  0.009330  0.040615  \n",
       "NOUN  0.043935  0.149134  0.012584  0.028825  0.009144  \n",
       "ADP   0.001266  0.008479  0.107062  0.034548  0.063275  \n",
       "ADV   0.014740  0.339022  0.130721  0.022886  0.029868  \n",
       ".     0.002790  0.089700  0.046137  0.025644  0.078219  \n",
       "PRON  0.014123  0.484738  0.070615  0.088383  0.006834  \n",
       "DET   0.000287  0.040247  0.206411  0.045134  0.022855  \n",
       "PRT   0.001174  0.401174  0.082975  0.012133  0.056751  \n",
       "VERB  0.030663  0.167956  0.066390  0.215930  0.022836  \n",
       "ADJ   0.011456  0.011456  0.063301  0.020971  0.021748  \n",
       "X     0.185086  0.206419  0.017682  0.075726  0.003075  \n",
       "NUM   0.026062  0.020707  0.035345  0.202428  0.184220  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Declare dimension for better readability\n",
    "print(f'Row:    Previous tag\\nColumn: Current tag')\n",
    "\n",
    "# Get transition matrix\n",
    "p_transition = compute_transition(train_data, tag_vocab)\n",
    "tags_df = pd.DataFrame(p_transition, columns = list(tags), index=list(tags))\n",
    "display(tags_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [1] (b) Compute the emission probability\n",
    "Additionally, we need to compute the emission probability, i.e. the probability that a POS (Part of Speech) tag is associated with a specific word: $p(y_t|x_t)$.\n",
    "\n",
    "For this purpose, we can just pre-compute the entire table of conditional probabilities based on the counts from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check total words in vocabulary\n",
    "words = {word for word,tag in train_data}\n",
    "word_vocab = {word: i for i, word in enumerate(words)}\n",
    "\n",
    "# Take a look\n",
    "# print(f'words: {words}')\n",
    "# print(f'word_vocab: {word_vocab}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_emission(train_data, tag_vocab, word_vocab):\n",
    "    # Initialize a matrix to store emission probabilities\n",
    "    p_emission = np.zeros((len(word_vocab), len(tag_vocab)))\n",
    "    \n",
    "    # Count word emissions for each POS tag in the training data\n",
    "    for i in range(len(train_data)):\n",
    "        # get current word and tag\n",
    "        current_word, current_tag = train_data[i]\n",
    "        \n",
    "        # convert to indices using the vocabularies\n",
    "        word_index = word_vocab[current_word]\n",
    "        tag_index = tag_vocab[current_tag]\n",
    "        \n",
    "        # increase emission count for current_word and current_tag\n",
    "        p_emission[word_index, tag_index] += 1\n",
    "    \n",
    "    # get column-wise sum of emission counts\n",
    "    row_sums = p_emission.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    # normalize counts to get probabilities\n",
    "    p_emission = p_emission / row_sums\n",
    "    \n",
    "    return p_emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONJ</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADV</th>\n",
       "      <th>.</th>\n",
       "      <th>PRON</th>\n",
       "      <th>DET</th>\n",
       "      <th>PRT</th>\n",
       "      <th>VERB</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>X</th>\n",
       "      <th>NUM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sacramento-based</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Orchestra</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dai-Ichi</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concept</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bundles</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>any</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01087</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.98913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CAT</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300-a-share</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reserve</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Old-House</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11052 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  CONJ  NOUN  ADP      ADV    .  PRON      DET  PRT  VERB   \n",
       "Sacramento-based   0.0   0.0  0.0  0.00000  0.0   0.0  0.00000  0.0   0.0  \\\n",
       "Orchestra          0.0   1.0  0.0  0.00000  0.0   0.0  0.00000  0.0   0.0   \n",
       "Dai-Ichi           0.0   1.0  0.0  0.00000  0.0   0.0  0.00000  0.0   0.0   \n",
       "concept            0.0   1.0  0.0  0.00000  0.0   0.0  0.00000  0.0   0.0   \n",
       "bundles            0.0   1.0  0.0  0.00000  0.0   0.0  0.00000  0.0   0.0   \n",
       "...                ...   ...  ...      ...  ...   ...      ...  ...   ...   \n",
       "any                0.0   0.0  0.0  0.01087  0.0   0.0  0.98913  0.0   0.0   \n",
       "CAT                0.0   1.0  0.0  0.00000  0.0   0.0  0.00000  0.0   0.0   \n",
       "300-a-share        0.0   0.0  0.0  0.00000  0.0   0.0  0.00000  0.0   0.0   \n",
       "Reserve            0.0   1.0  0.0  0.00000  0.0   0.0  0.00000  0.0   0.0   \n",
       "Old-House          0.0   1.0  0.0  0.00000  0.0   0.0  0.00000  0.0   0.0   \n",
       "\n",
       "                  ADJ    X  NUM  \n",
       "Sacramento-based  1.0  0.0  0.0  \n",
       "Orchestra         0.0  0.0  0.0  \n",
       "Dai-Ichi          0.0  0.0  0.0  \n",
       "concept           0.0  0.0  0.0  \n",
       "bundles           0.0  0.0  0.0  \n",
       "...               ...  ...  ...  \n",
       "any               0.0  0.0  0.0  \n",
       "CAT               0.0  0.0  0.0  \n",
       "300-a-share       1.0  0.0  0.0  \n",
       "Reserve           0.0  0.0  0.0  \n",
       "Old-House         0.0  0.0  0.0  \n",
       "\n",
       "[11052 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p_emission = compute_emission(train_data, tag_vocab, word_vocab)\n",
    "words_df = pd.DataFrame(p_emission, columns = list(tags), index=list(words))\n",
    "display(words_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONJ</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADV</th>\n",
       "      <th>.</th>\n",
       "      <th>PRON</th>\n",
       "      <th>DET</th>\n",
       "      <th>PRT</th>\n",
       "      <th>VERB</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>X</th>\n",
       "      <th>NUM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>most</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.509091</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.490909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spending</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marks</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>black</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buffet</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007181</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.992819</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>giant</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slide</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diversified</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>any</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.989130</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>602 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             CONJ      NOUN  ADP       ADV    .  PRON       DET  PRT   \n",
       "most          0.0  0.000000  0.0  0.509091  0.0   0.0  0.000000  0.0  \\\n",
       "spending      0.0  0.950000  0.0  0.000000  0.0   0.0  0.000000  0.0   \n",
       "marks         0.0  0.888889  0.0  0.000000  0.0   0.0  0.000000  0.0   \n",
       "black         0.0  0.333333  0.0  0.000000  0.0   0.0  0.000000  0.0   \n",
       "buffet        0.0  0.500000  0.0  0.000000  0.0   0.0  0.000000  0.0   \n",
       "...           ...       ...  ...       ...  ...   ...       ...  ...   \n",
       "The           0.0  0.007181  0.0  0.000000  0.0   0.0  0.992819  0.0   \n",
       "giant         0.0  0.200000  0.0  0.000000  0.0   0.0  0.000000  0.0   \n",
       "slide         0.0  0.800000  0.0  0.000000  0.0   0.0  0.000000  0.0   \n",
       "diversified   0.0  0.000000  0.0  0.000000  0.0   0.0  0.000000  0.0   \n",
       "any           0.0  0.000000  0.0  0.010870  0.0   0.0  0.989130  0.0   \n",
       "\n",
       "                 VERB       ADJ    X  NUM  \n",
       "most         0.000000  0.490909  0.0  0.0  \n",
       "spending     0.050000  0.000000  0.0  0.0  \n",
       "marks        0.111111  0.000000  0.0  0.0  \n",
       "black        0.000000  0.666667  0.0  0.0  \n",
       "buffet       0.500000  0.000000  0.0  0.0  \n",
       "...               ...       ...  ...  ...  \n",
       "The          0.000000  0.000000  0.0  0.0  \n",
       "giant        0.000000  0.800000  0.0  0.0  \n",
       "slide        0.200000  0.000000  0.0  0.0  \n",
       "diversified  0.333333  0.666667  0.0  0.0  \n",
       "any          0.000000  0.000000  0.0  0.0  \n",
       "\n",
       "[602 rows x 12 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at words that with multiple possible tags, to ensure that the function works \"fine\"\n",
    "\n",
    "# filter dataframe for rows without a 1 (in this case words that can have different tags)\n",
    "filtered_words_df = words_df[~words_df.eq(1).any(axis=1)]\n",
    "\n",
    "# words_df.eq(1): create boolean DataFrame of shape(words_df) where ones are marked as True others as False\n",
    "# .any(axis=1): for each row, check if any element is True\n",
    "# ~: bitwise NOT operator\n",
    "filtered_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CONJ    0.000000\n",
       "NOUN    0.857143\n",
       "ADP     0.000000\n",
       "ADV     0.142857\n",
       ".       0.000000\n",
       "PRON    0.000000\n",
       "DET     0.000000\n",
       "PRT     0.000000\n",
       "VERB    0.000000\n",
       "ADJ     0.000000\n",
       "X       0.000000\n",
       "NUM     0.000000\n",
       "Name: fine, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example interpretation: fine\n",
    "# fine can be an adverb, e.g. \"I am fine\"\n",
    "# it could also be used as a noun, e.g. \"I had to pay a fine for paking in the wrong spot\"\n",
    "\n",
    "display(words_df.loc['fine'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [1] (c) Compute the unigram probabilities\n",
    "Finally, we need to compute the unigram probability of observing a given word: $p(y)$.\n",
    "Analogously for a given POS-tag: $p(x)$\n",
    "\n",
    "Again, we compute this probability on the counts from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_unigrams(train_data, word_vocab, tag_vocab):\n",
    "    p_y = np.zeros(len(word_vocab))\n",
    "    p_x = np.zeros(len(tag_vocab))\n",
    "    \n",
    "    # count words and POS tags in the training data\n",
    "    for word, tag in train_data:\n",
    "        if word in word_vocab:\n",
    "            p_y[word_vocab[word]] += 1\n",
    "        if tag in tag_vocab:\n",
    "            p_x[tag_vocab[tag]] += 1\n",
    "    \n",
    "    # normalize counts to get probabilities\n",
    "    p_y = p_y / p_y.sum()\n",
    "    p_x = p_x / p_x.sum()\n",
    "    \n",
    "    return p_y, p_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p_y, p_x = compute_unigrams(train_data, word_vocab, tag_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02268709, 0.28596688, 0.09839372, 0.03210061, 0.11606276,\n",
       "       0.02733159, 0.08662682, 0.03181422, 0.135226  , 0.06412651,\n",
       "       0.06478645, 0.03487735])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sacramento-based</th>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Orchestra</th>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dai-Ichi</th>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concept</th>\n",
       "      <td>0.000062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bundles</th>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>any</th>\n",
       "      <td>0.001146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CAT</th>\n",
       "      <td>0.000050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300-a-share</th>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reserve</th>\n",
       "      <td>0.000075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Old-House</th>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11052 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0\n",
       "Sacramento-based  0.000012\n",
       "Orchestra         0.000012\n",
       "Dai-Ichi          0.000012\n",
       "concept           0.000062\n",
       "bundles           0.000012\n",
       "...                    ...\n",
       "any               0.001146\n",
       "CAT               0.000050\n",
       "300-a-share       0.000025\n",
       "Reserve           0.000075\n",
       "Old-House         0.000012\n",
       "\n",
       "[11052 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CONJ</th>\n",
       "      <td>0.022687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>0.285967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>0.098394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>0.032101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.116063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>0.027332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>0.086627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRT</th>\n",
       "      <td>0.031814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>0.135226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>0.064127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0.064786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>0.034877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0\n",
       "CONJ  0.022687\n",
       "NOUN  0.285967\n",
       "ADP   0.098394\n",
       "ADV   0.032101\n",
       ".     0.116063\n",
       "PRON  0.027332\n",
       "DET   0.086627\n",
       "PRT   0.031814\n",
       "VERB  0.135226\n",
       "ADJ   0.064127\n",
       "X     0.064786\n",
       "NUM   0.034877"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p_y, p_x = compute_unigrams(train_data, word_vocab, tag_vocab)\n",
    "words_df = pd.DataFrame(p_y, index=list(words))\n",
    "display(words_df)\n",
    "tag_df = pd.DataFrame(p_x, index=list(tags))\n",
    "display(tag_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we get 0.000012 surprisingly often in our as a word share. Mistake ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add an unknown token:\n",
    "Now we computed these probabilities from the training data. Unfortunately evaluating on unseen data can lead to problems if we encounter words that we did not see before. There are many ways to deal with this. One option is to replace unseen words with an `<UNK>` token. We set the emission probability to be uniform, s.t. the algorithm has to rely on the transition probability to figure out the POS-tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps= 1e-12\n",
    "unk_id = len(word_vocab)\n",
    "word_vocab[\"<UNK>\"] = unk_id\n",
    "p_y -= eps\n",
    "p_y = np.concatenate([p_y,np.array([p_y.shape[0] * eps])], axis=0)\n",
    "p_emission = np.concatenate([p_emission,np.ones_like(p_emission[:1])/len(p_emission[0])], axis=0)\n",
    "def turn_unk(w):\n",
    "    if w not in word_vocab:\n",
    "        return \"<UNK>\"\n",
    "    else:\n",
    "        return w\n",
    "test_data = [(turn_unk(w),t) for w,t in test_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] 2. Numerical Stability\n",
    "We have now computed all the probabilities that we need to apply the algorithm from the lecture.\n",
    "However, if we just apply this algorithm naively, we will run into problems of numerical stability. \n",
    "To illustrate this, let's compute the probability of a random sequence of tags $p(x_0,x_1,...,x_N)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13522599925289502\n"
     ]
    }
   ],
   "source": [
    "N = 100 \n",
    "idx = ... # TODO\n",
    "p_seq = ... # TODO\n",
    "print(p_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, although we are multiplying non-zero probabilities, we end up with a probability of zero very quickly as we increase $N$. \n",
    "\n",
    "This problem can be avoided by performing computations in log-space. This means that we take the logarithm of all probability values before peforming any computations with them. \n",
    "This will turn all products into summations and divisions into subtractions. \n",
    "Let's try this trick on the task above and compute the value of $\\log p(x_0,x_1,...,x_N)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_p_seq = ... # TODO\n",
    "print(log_p_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we convert this back to $p(x_0,...,x_N)$, by taking the exponential, we will still end up with zero. So what does this help us? \n",
    "\n",
    "Here we can make use of the fact that the logarithm is a monotone increasing function, which means in particular that \n",
    "$$\n",
    "\\mathrm{argmax}_x p(x) = \\mathrm{argmax}_x \\log p(x)\n",
    "$$ \n",
    "This is useful in cases like this one where the exact probability is not of interest, but we just want to know the position of the maximum value of a particular function. \n",
    "\n",
    "Finally, we need to deal with the situation where we want to take the logarithm of a sum of probabilities. \n",
    "In that case, we cannot just \"pull the log through\". \n",
    "Instead we need to use the log-sum-exp trick:\n",
    "$$\n",
    "\\log \\sum_{i=1}^{M} p(x_i) = b + \\log \\sum_{i=1}^M \\exp ((\\log p(x_i)) - b)   \n",
    "$$\n",
    "with $b = \\max_i \\log p(x_i)$. \n",
    "Please read this short explanation to understand why/how this works: [http://wittawat.com/posts/log-sum_exp_underflow.html](http://wittawat.com/posts/log-sum_exp_underflow.html)\n",
    "\n",
    "In the following implementation, we will exclusively work with log-probabilities and use the log-sum-exp trick wherever we encounter a sum.\n",
    "To prepare for this, we now convert all probabilities into log-probabilities and add a small `eps` term to all values to avoid $\\log(0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps= 1e-12\n",
    "log_p_y = np.log(p_y + eps)\n",
    "log_p_x = np.log(p_x + eps)\n",
    "log_p_transition = np.log(p_transition + eps)\n",
    "log_p_emission = np.log(p_emission + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The algorithm\n",
    "Now that we have implemented the emission and transition probabilities, we can use them to compute the predictions $p(x_t|Y)$ for all $t=0,\\ldots,n$ by applying the algorithm from the lecture.\n",
    "Note that we $x$ is a discrete variable (we only have a limited vocabulary $\\mathcal{X}$), which allows us to evaluate the integral expressions by carrying out a sum.\n",
    "\n",
    "#### Please read all subtasks carefully before starting your implementation! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [2] (a) Predict: \n",
    "Please compute the prediction step here. Note that you need to apply the log-sum-exp trick here and use numpy operations to efficiently compute the probabilities for all possible $x_t$ at the current timestep $t$. \n",
    "\n",
    "$$ p(x_t|Y_{0:t-1}) = \\sum_{x_{t-1} \\in \\mathcal{X}} p(x_t|x_{t-1}) p(x_{t-1}|Y_{0:t-1}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(t, log_p_update, log_p_transition):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        t (int): Current timestep.\n",
    "        log_p_update (np.array): `log p(x_t|y_{0:t})` array of shape [T+1,len(tag_vocab)].\n",
    "        log_p_transition (np.array): `log p(x_t|x_{t-1})` array of shape [len(tag_vocab),len(tag_vocab)].\n",
    "    \n",
    "    Returns: \n",
    "        np.array: `log p(x_t|y_{0:t-1})` for timestep t. Array of shape [len(tag_vocab)]\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [2] (b) Update: \n",
    "Please compute the update step here. Note that you need to perform the operations in log-space here and use numpy operations to efficiently compute the probabilities for all possible $x_t$ at the current timestep $t$. \n",
    "\n",
    "$$ p(x_t|Y_{1:t}) = p(y_t|x_t) \\frac{p(x_t|Y_{0:t-1})}{p(y_t)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(Y, t, log_p_predict, log_p_y, log_p_emission):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        Y (np.array): Observed sequence of words in array of shape [T].\n",
    "        t (int): Current timestep.\n",
    "        log_p_predict (np.array): `log p(x_t|y_{0:t-1})` array of shape [T+1,len(tag_vocab)].\n",
    "        log_p_y (np.array): `log p(y_t)` array of shape [len(word_vocab)].\n",
    "        log_p_emission (np.array): `log p(y_t|x_t)` array of shape [len(word_vocab),len(tag_vocab)].\n",
    "    \n",
    "    Returns: \n",
    "        np.array: `log p(x_t|y_{0:t})` for timestep t. Array of shape [len(tag_vocab)]\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [2] (c) Smoothing: \n",
    "Please compute the smoothing step here. Note that you need to apply the log-sum-exp trick here and use numpy operations to efficiently compute the probabilities for all possible $x_t$ at the current timestep $t$. \n",
    "\n",
    "$$ p(x_t|Y) = p(x_t|Y_{0:t}) \\sum_{x_{t+1} \\in \\mathcal{X}} p(x_{t+1}|x_t) \\frac{p(x_{t+1}|Y)}{ p(x_{t+1}| Y_{0:t}) } $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing(t, log_p_predict, log_p_update, log_p_marginal, log_p_transition): \n",
    "    \"\"\"\n",
    "    Args:\n",
    "        t (int): Current timestep.\n",
    "        log_p_predict (np.array): `log p(x_t|y_{0:t-1})` array of shape [T+1,len(tag_vocab)].\n",
    "        log_p_update (np.array): `log p(x_t|y_{0:t})` array of shape [T+1,len(tag_vocab)].\n",
    "        log_p_marginal (np.array): `log p(x_t|Y)` array of shape [T+1,len(tag_vocab)].\n",
    "        log_p_transition (np.array): `log p(x_t|x_{t-1})` array of shape [len(tag_vocab),len(tag_vocab)].\n",
    "    \n",
    "    Returns: \n",
    "        np.array: `log p(x_t|Y)` for timestep t. Array of shape [len(tag_vocab)] \n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plugging it together\n",
    "The functions above are applied in the following function that executes our algorithm to compute the marginals.\n",
    "Please do not change anything in this function or the signature of the functions above. \n",
    "Take a look at their docstrings to understand what you need to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_marginals(Y_, log_p_transition, log_p_emission, log_p_y, log_p_x, tag_vocab):\n",
    "    T = len(Y_)\n",
    "    Y = np.zeros(T+1, dtype=int)\n",
    "    Y[1:] = Y_ # since we are counting time from 1\n",
    "    log_p_predict = np.zeros((T+1,len(tag_vocab)))\n",
    "    log_p_update = np.zeros((T+1,len(tag_vocab)))\n",
    "    log_p_update[0] = log_p_x  \n",
    "    for t in range(1,T+1):\n",
    "        log_p_predict[t] = predict(t, log_p_update, log_p_transition)\n",
    "        log_p_update[t] = update(Y, t, log_p_predict, log_p_y, log_p_emission)\n",
    "        \n",
    "    log_p_marginal = np.zeros((T+1,len(tag_vocab)))\n",
    "    for t in range(T-1,0,-1):\n",
    "        log_p_marginal[t] = smoothing(t, log_p_predict, log_p_update, log_p_marginal, log_p_transition) \n",
    "    return log_p_marginal[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate \n",
    "Now we can evaluate the algorithm we implemented on the test data.\n",
    "\n",
    "Hint: you should get an accuracy > 90% if your implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tokens(X, vocab):\n",
    "    inv_map = {v: k for k, v in vocab.items()}\n",
    "    return [inv_map[x] for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y_test = np.array([word_vocab[w] for w,t in test_data])\n",
    "X_test = np.array([tag_vocab[t] for w,t in test_data])\n",
    "p_X_Y = compute_marginals(Y_test, log_p_transition, log_p_emission, log_p_y, log_p_x, tag_vocab)\n",
    "X_pred = np.argmax(p_X_Y, axis=1)\n",
    "acc = np.sum(X_pred == X_test) / X_pred.shape[0] * 100\n",
    "print(to_tokens(X_test[:10], tag_vocab))\n",
    "print(to_tokens(X_pred[:10], tag_vocab))\n",
    "print(to_tokens(Y_test[:10], word_vocab))\n",
    "print(f\"Accuracy: {acc}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
